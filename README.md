# Visualizing-Deep-Neural-Networks
This project was done as part of the EECE 5642: Data Visualization course by Raksha Ramkumar, Balaji Sundershan and Nihal Mathew Sashikumar. The main goal of this project is to implement Gradient Class Activation Map (Grad-CAM), a visualization tool that aids in the interpretation of deep neural networks.

## Introduction
Deep Learning techniques are at the forefront of solving many computer vision problems across
 various industries, from healthcare to automotive to environmental studies. These powerful techniques,
 though effective, do not offer the user a glimpse into the patterns that they are learning in order to
 make a particular decision. In order to build trust in intelligent systems, we must build _transparent_ deep learning models and
 visualizing them through the Grad-CAM gives a peek into the _black box_ that these systems are. 

We apply Grad-CAM on three computer-vision tasks: Image Classification, Image Captioning and Visual Question-Answering. Using gradient-based localization "visual explanations" are produced for decisions for these CNN-based models. 

## Gradient Class Activation Mapping (Grad-CAM)
 Grad-CAM is a generalized version of the Class Activation Mapping to visualize the learning of the
 model. This technique identifies discriminative regions in the image leading to a particular output,
 without modifying the network architecture or re-training the model.

 The main idea is that the gradient of the score for class c is calculated with respect to the feature
 map activation of the convolutional layers before the softmax. These back-propagating gradients are
 averaged over the width and height dimensions of that particular layer. The output of the process is an
 averaged gradient map where the pixels having positive and higher gradients contribute more to the
 decision being made. This heatmap is overlaid on the original image to understand the visualization
 with respect to the original image.

 The following is the result of the Grad-CAM applied for a _wardrobe_ image when inferenced using the VGG-16, which is a classification model:
![image](https://github.com/RakshaRamkumar/Visualizing-Deep-Neural-Networks/assets/63054940/1c79a93b-69e7-4ff5-b786-5a114d5670d2)

## Results 
### Image Classification
Applying Grad-CAM for the Image Classification task helps us to visually analyse and validate model and data bias in
 prediction performed by different models for the same ground truth. Visualizing through deep layers
 verifies whether our network is looking at the right patterns in the image and activating around the
 patterns. These activated patterns in the concentrated gradient maps highlights the important regions
 in the image used for classification. The figure displayed below is the implementation of Grad-CAM
 using pre-trained image classification models with the ground truth image on the first column.

 ![image](https://github.com/RakshaRamkumar/Visualizing-Deep-Neural-Networks/assets/63054940/faa0f3dc-708e-4ebf-bccc-d821664b9e28)

From left to right: ground truth, Alexnet, VGG16 and Resnet50 images with their respective label below the image

The ground truth of the first image in Figure 3 is ‘Goldfish’. Alexnet and VGG16 were correctly able
 to classify the image whereas Resnet50 misclassified it as ’Hook’. Here comes the visual attention
 maps which helps us to investigate the model. When the attention maps of all three models are
 analyzed, it’s evident that the attention maps of Alexnet and VGG16 highlights (high concentrated
 yellow region) at the prominent features of goldfish whereas on the other hand Resnet50 highlights
 the background features below the fish. Another example is the image in the second row of Figure
 3, where the ground truth is ‘Wardrobe’. Here VGG16 and Resnet50 correctly classifies the image
 whereas Alexnet misclassifies it as a medicine chest. When we investigate the attention maps,
 we could see that VGG16 and Resnet50 highlights the features at the wardrobe whereas Alexnet
 highlights other features below and adjacent to the wardrobe.

 ### Image Captioning
Three architectures- AlexNet, ResNet-18 and
 ResNet-152 were trained for Image Captioning and their activation map was visualized using Grad-CAM. The
 architecture with less layers capture smaller receptive field and architecture with more layers capture
larger receptive field. Smaller receptive field captures small context in the image and larger receptive field captures larger context in the image.

To generate image captions correctly, the model has to see larger context in the image. As can be seen
 from Figure 4, captions generated by ResNet152 is more accurate compared to captions generated by
 AlexNet and ResNet18.

 ![image](https://github.com/RakshaRamkumar/Visualizing-Deep-Neural-Networks/assets/63054940/da13e919-cc1a-4ae0-96eb-aad301e5a5f0)

 From left to right: ground truth, AlexNet, ResNet18 and ResNet152 images with their
 respective label below the image

### Visual Question Answering
For this task, the architectures VGG16, ResNet
50 and ResNet-152 were trained. Due to the reasons mentioned above, we can see from the figure that VGG16 performs
 well on questions related to smaller context and ResNet-152 performs well on questions related to
 larger context.

 ![image](https://github.com/RakshaRamkumar/Visualizing-Deep-Neural-Networks/assets/63054940/2e53cf23-2bc4-417a-b688-4ba1a786bed8)

 From left to right: ground truth, VGG16, ResNet50 and ResNet152 images with their
 respective label below the image.
 
